<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Enhancing Navigational Scene Understanding via Integrated Language Models in Maritime Environment
</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo_cocaptain.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Leveraging Integrated Language Models for Enhanced Navigational Scene Understanding in a Complex Canal Environment
</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yeongha Shin</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jinwhan Kim</a><sup>2*</sup></span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank"></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Korea Advanced Institute of Science and Technology (KAIST) <br>IROS 2025 review</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this study, we introduce an innovative algorithm for enhanced navigational scene understanding in complex canal environments by utilizing large language models (LLM) and visual language models (VLM) to achieve autonomous maritime situational awareness. The proposed algorithm interprets the meanings of various features and marks on detected objects in maritime contexts. By combining this information with radar and camera data, the algorithm generates cost maps for safe navigation. This approach offers two key benefits: (1) the ability to identify navigable areas considering obstacles, maritime marks, rules, and ship intentions, and (2) decision-making support based on reasoning, bridging the information gap between human operators and perception results. The performance of the proposed approach is demonstrated using a real-world dataset.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
  
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/flowchart_2.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Flowchart of the proposed algorithm. The algorithm has three stages: Stage I detects objects with a predefined class, followed by coordinate transformation and tracking. Stage II detects maritime marks using VLM and applies LLM for scene understanding. In Stage III, the data is integrated to create a Scene Understanding Cost Map for safe path planning and logical inferences.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Perception of Extrinsic Features</h3>         
          <!-- 이미지 추가 -->
          <figure class="image" style="margin-bottom: 0;"> 
            <img src="static/images/Figure1.png" alt="Image Description" style="width:100%; height:auto;">
            <figcaption style="text-align: center; font-size: 14px; color: #666; margin-top: 0;">
              In maritime environments, where proactive avoidance is essential, detecting distant objects is crucial. We used the RT-DETR and YOSO models for precise detection. RT-DETR excels in detecting distant boats, while YOSO provides pixel-level segmentation of land and bridge structures.
              The proposed algorithm integrates image data into the radar coordinate system using extrinsic and intrinsic parameters of the camera.
              Fig. 2(a) shows accurate boat detection, while (b) demonstrates the correct generation of land and bridge information. The results are displayed in Fig. 2(c), where boat detection and segmentation of bridges and land are accurately aligned with the radar coordinates. 
            </figcaption>
          </figure>

          <h3 class="title is-4">Perception of Intrinsic Features</h3>
          <h4 class="title is-4">VLM Prompt Detection</h4>
          <!-- 이미지 추가 -->
          <figure class="image" style="margin-bottom: 0;">
            <img src="static/images/Figure2.png" alt="Image Description" style="max-width:100%; height: auto;">
            <figcaption style="text-align: center; font-size: 14px; color: #666; margin-top: 0;">
              To detect complex objects in marine situations, we employed the VLM model, specifically Grounding DINO. This model uses free-form prompts to perform detection for additional object classes. 
            </figcaption>
          </figure>

          <h4 class="title is-4">LLM Navigational Scene Understanding</h4>
          <!-- 이미지 추가 -->
          <figure class="image" style="margin-bottom: 0;">
            <img src="static/images/Figure3.png" alt="Image Description" style="max-width:100%; height: auto;">
            <figcaption style="text-align: center; font-size: 14px; color: #666; margin-top: 0;">
              In the LLM Navigational Scene Understanding module, object detection results from Stage I and VLM prompt detection from Stage II are used for maritime scene understanding. GPT-4o was chosen for its ability to handle visual referring prompting and complex environments. 
            </figcaption>
          </figure>
          
          <h3 class="title is-4">Scene Understanding Cost Map</h3>         
          <!-- 이미지 추가 -->
          <figure class="image" style="margin-bottom: 0;">
            <img src="static/images/Figure4.png" alt="Image Description" style="max-width:100%; height: auto;">
            <figcaption style="text-align: center; font-size: 14px; color: #666; margin-top: 0;">
              The results of the VLM and LLM in Stage II are reflected as follows: Maritime marks, such as bridge and buoy marks, and ship intentions are incorporated into obstacle zones with the probability.
              An occupancy grid map was generated based on the results from Stages I and II using the occupancy grid map model. As shown in Fig. 5(b), the bridge pillars visible from the camera are precisely detected as obstacle areas, and the area between the bridge pillars is designated as a navigable zone due to the camera’s ability to detect height. This allows for flexible application according to the given situation. Fig. 5(c) illustrates this, where maritime marks, ship intentions, and rules are integrated, marking the left-side route as unnavigable. 
            </figcaption>
          </figure>

          <!-- 이미지와 설명이 끝나는 부분 -->
        </div>
      </div>
    </div>
  </div>
</section>
  
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/vlmllm_resize.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Experiment Results of the proposed algorithm.  
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{shin2024llmship,
  title={Enhancing Navigational Scene Understanding via Integrated Language Models in Maritime Environment},
  author={Shin, Yeongha and Kim, Jinwhan},
  booktitle={},
  year={2025},
  organization={},
  note={Robotics Program, Korea Advanced Institute of Science and Technology (KAIST)}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
